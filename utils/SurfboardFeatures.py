# -*- coding: utf-8 -*-
"""SurfboardFeatures

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YC5JRyMZCHNWbpTAO6Fq9OmZhuU6M1MK
"""

import librosa
import numpy as np
import scipy
from sklearn.decomposition import PCA
import random
from tensorflow.keras import layers, datasets, models, Model
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential
from tensorflow import keras
import scipy
from tensorflow.keras.layers import Dense, Flatten, InputLayer, Input
from tqdm import tqdm
import sklearn
import surfboard
from surfboard.sound import Waveform
from surfboard.feature_extraction import extract_features

from utils import Parameters

SAMPLING_RATE = Parameters.SAMPLING_RATE
DURATION = Parameters.DURATION
NUM_MELS = Parameters.NUM_MELS
FMAX = Parameters.FMAX
TOP_DB = Parameters.TOP_DB
EPS = Parameters.EPS
N_MFCC = Parameters.N_MFCC
PCA_COMPONENTS = Parameters.PCA_COMPONENTS
components_list = Parameters.components_list
statistics_list = Parameters.statistics_list

def myfunction():
  """
  Returns the value 0.35 as an input to the random.shuffle() function
  inputs:
    - N/A
  outputs:
    - the number 0.35
  """
  return 0.35

def GenerateVectors(path, dataset_type, LARGE_STATS=True):
  """
  Computes the normalized feature vectors of the input path
  inputs:
    - path: the path containing all the desired signals to process
    - dataset_type: the type of the dataset (train/test/val)
    - LARGE_STATS: a boolean to decide whether to computed detailed statistics
      set to True default
  outputs:
    - x_2: a numpy array containing all the feature vector of the signals contained
      in the input
  """

  positive_paths = []
  negative_paths = []

  #Iterates over positive files in the path and appends them to positive_paths
  for entry in os.scandir(os.path.join(path, os.path.join(dataset_type, 'pos'))):
    if entry.is_file():
        #if librosa.get_duration(filename=entry) > 0:
      positive_paths.append(entry.path)
  
  #Iterates over negative files in the path and appends to negative_paths
  for entry in os.scandir(os.path.join(path, os.path.join(dataset_type, 'neg'))):
    if entry.is_file():
        #if librosa.get_duration(filename=entry) > 0:
      negative_paths.append(entry.path)
        
  #Computes the normalized feature vectors of the positive waveforms
  waveforms_positive = []
  for paths in tqdm(positive_paths):
    waveforms_positive.append(Waveform(path=paths, sample_rate=SAMPLING_RATE))
  postive_features_df = extract_features(waveforms=waveforms_positive, components_list=components_list, statistics_list=statistics_list)
  positive_features = sklearn.preprocessing.normalize(postive_features_df.to_numpy(), axis=0)

  #Computes the normalized feature vectors of the negative signals
  waveforms_negative = []
  for npaths in tqdm(negative_paths):
    waveforms_negative.append(Waveform(path=npaths, sample_rate=SAMPLING_RATE))
  negative_features_df = extract_features(waveforms=waveforms_negative, components_list=components_list, statistics_list=statistics_list)
  negative_features = sklearn.preprocessing.normalize(negative_features_df.to_numpy(), axis=0)

  #Combines the two feature vector lists
  positive_features_list = positive_features.tolist()
  negative_features_list = negative_features.tolist()

  x_2_list = positive_features_list.copy()
  x_2_list.extend(negative_features_list)

  #Shuffle the list
  random.shuffle(x_2_list, myfunction)

  x_2 = np.array(x_2_list)
  
  return x_2  


def Reduce_Dim(input):
  """
  Performs dimensionality reduction via PCA and reshaping
  inputs:
    - input: the input feature vector to be dimensionality reduced
  outputs:
    - projected: the PCA reduced feature vector (after reshaping and transforms)
  """
  pca = PCA(n_components=PCA_COMPONENTS)
  input_2 = input.copy()
  input_2 = input_2.reshape(input.shape[0], input.shape[1]) # n items each of dim 885 to be fed for PCA
  projected = pca.fit_transform(input_2)

  return projected